{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf883e6e-e5ab-410f-b48e-7324e0b61d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from transformers import pipeline\n",
    "\n",
    "# Import CountVectorizer and TFIDFVectorizer from feature_extraction.text.\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be7dd7d6-65fe-4b78-8fd8-32dda8916017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in 1st dataset\n",
    "news1 = pd.read_csv('./data/articles1.csv')\n",
    "news1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5faab7c-aa91-4909-ad72-94ac6e58d4cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49999, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in 2nd dataset\n",
    "news2 = pd.read_csv('./data/articles2.csv')\n",
    "news2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5fbaec8-84fe-4c2f-a665-feaabbcc2ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42571, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in 3rd dataset\n",
    "news3 = pd.read_csv('./data/articles3.csv')\n",
    "news3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e5be648-9e0b-4c5f-87aa-1851dea37ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142570, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate datasets\n",
    "frames = [news1, news2, news3]\n",
    "\n",
    "df = pd.concat(frames)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5a90398-3f0a-4a85-8774-06f367b144cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index of datasets (had to do this otherwise when dropping publications it would take out all the rows with the same index since there was index overlap)\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "747fadce-f116-43fc-a262-d63230bcca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unecessary columns from dataset\n",
    "df.drop(columns=['index', 'Unnamed: 0', 'id', 'url'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cc6efb8-4256-4dde-8b8a-23c0faec7892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unneeded publications from dataset\n",
    "df.drop(df.loc[df['publication'] == 'Business Insider'].index, inplace = True)\n",
    "df.drop(df.loc[df['publication'] =='Talking Points Memo'].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1971c043-079c-406e-8661-b5c8ea6512df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast title to str (there was a float title)\n",
    "df['title'] = df['title'].astype(str)\n",
    "\n",
    "# cast date to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcb17e28-c1f5-4ced-8add-01c2a6c5f623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Breitbart          23781\n",
       "New York Post      17493\n",
       "NPR                11992\n",
       "CNN                11488\n",
       "Washington Post    11114\n",
       "Reuters            10710\n",
       "Guardian            8681\n",
       "New York Times      7803\n",
       "Atlantic            7179\n",
       "National Review     6203\n",
       "Vox                 4947\n",
       "Buzzfeed News       4854\n",
       "Fox News            4354\n",
       "Name: publication, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking value counts of news sources\n",
    "df['publication'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da0517c6-5e50-4da4-a6dc-8f819bc31d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing our text processors\n",
    "sent = SentimentIntensityAnalyzer()\n",
    "tokenizer_1 = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74f94bb3-f9a4-4325-840c-464a18a25aff",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-379f21ede237>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# applying sentiment analyis to the content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mldf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sent_content'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mldf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mldf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'neg_c'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'neg'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mldf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sent_content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mldf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'neu_c'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'neu'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mldf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sent_content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4136\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4137\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4138\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/sentiment/vader.py\u001b[0m in \u001b[0;36mpolarity_scores\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \"\"\"\n\u001b[1;32m    359\u001b[0m         \u001b[0;31m# text, words_and_emoticons, is_cap_diff = self.preprocess(text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m         sentitext = SentiText(text, self.constants.PUNC_LIST,\n\u001b[0m\u001b[1;32m    361\u001b[0m                               self.constants.REGEX_REMOVE_PUNCTUATION)\n\u001b[1;32m    362\u001b[0m         \u001b[0msentiments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/sentiment/vader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, text, punc_list, regex_remove_punctuation)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPUNC_LIST\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpunc_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREGEX_REMOVE_PUNCTUATION\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregex_remove_punctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords_and_emoticons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_words_and_emoticons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;31m# doesn't separate words from\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;31m# adjacent punctuation (keeps emoticons & contractions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/sentiment/vader.py\u001b[0m in \u001b[0;36m_words_and_emoticons\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \"\"\"\n\u001b[1;32m    305\u001b[0m         \u001b[0mwes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m         \u001b[0mwords_punc_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_words_plus_punc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m         \u001b[0mwes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwe\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwes\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/sentiment/vader.py\u001b[0m in \u001b[0;36m_words_plus_punc\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mwords_only\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords_only\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0;31m# the product gives ('cat', ',') and (',', 'cat')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0mpunc_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPUNC_LIST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mpunc_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPUNC_LIST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mwords_punc_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpunc_before\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/sentiment/vader.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mwords_only\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords_only\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0;31m# the product gives ('cat', ',') and (',', 'cat')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0mpunc_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPUNC_LIST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mpunc_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPUNC_LIST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mwords_punc_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpunc_before\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# diving our dataset by publication and saving it to a dictionary where the key is the publication and the value is it's dataframe\n",
    "t0 = time.time()\n",
    "data_dict = {}\n",
    "for i in df['publication'].value_counts().index:\n",
    "    # creating each individual dataframe with a standardized name and resetting the index\n",
    "    ldf = i.replace(' ', '').lower()\n",
    "    data_dict[ldf] = pd.DataFrame(df.loc[df['publication'] == i])\n",
    "    data_dict[ldf].reset_index(inplace = True)\n",
    "    data_dict[ldf].drop(columns = 'index', inplace= True)\n",
    "    \n",
    "    # applying sentiment analyis to the content\n",
    "    data_dict[ldf]['sent_content'] = data_dict[ldf]['content'].apply(sent.polarity_scores)\n",
    "    data_dict[ldf]['neg_c'] = [d.get('neg') for d in data_dict[ldf]['sent_content']]\n",
    "    data_dict[ldf]['neu_c'] = [d.get('neu') for d in data_dict[ldf]['sent_content']]\n",
    "    data_dict[ldf]['pos_c'] = [d.get('pos') for d in data_dict[ldf]['sent_content']]\n",
    "    data_dict[ldf]['comp_c'] = [d.get('compound') for d in data_dict[ldf]['sent_content']]\n",
    "    \n",
    "    # tokenizing and lemmatizing the content\n",
    "    data_dict[ldf]['tokens_c'] = data_dict[ldf]['content'].apply(tokenizer_1.tokenize)\n",
    "    data_dict[ldf]['lem_c'] = [[lemmatizer.lemmatize(token) for token in l] for l in data_dict[ldf]['tokens_c'] ]\n",
    "    \n",
    "    # applying sentiment analysis to the title\n",
    "    data_dict[ldf]['sent_title'] = data_dict[ldf]['title'].apply(sent.polarity_scores)\n",
    "    data_dict[ldf]['neg_t'] = [d.get('neg') for d in data_dict[ldf]['sent_title']]\n",
    "    data_dict[ldf]['neu_t'] = [d.get('neu') for d in data_dict[ldf]['sent_title']]\n",
    "    data_dict[ldf]['pos_t'] = [d.get('pos') for d in data_dict[ldf]['sent_title']]\n",
    "    data_dict[ldf]['comp_t'] = [d.get('compound') for d in data_dict[ldf]['sent_title']]\n",
    "    \n",
    "    # tokenizing and lemmatizing the title\n",
    "    data_dict[ldf]['tokens_t'] = data_dict[ldf]['title'].apply(tokenizer_1.tokenize)\n",
    "    data_dict[ldf]['lem_t'] = [[lemmatizer.lemmatize(token) for token in l] for l in data_dict[ldf]['tokens_t'] ]\n",
    "\n",
    "print(f'Creating the Data Dictionary took {time.time() - t0} seconds to run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa007cc-6eec-487a-bf42-c4c5d5a9b76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputting our data separate files for each publication\n",
    "for i in data_dict:\n",
    "    data_dict[i].to_csv(f'./data/{i}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ace3868-4145-43a8-a4b6-d5f56cf79a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of the publication names\n",
    "\n",
    "names = ['buzzfeednews','cnn', 'vox',\n",
    "         'guardian', 'atlantic', 'washingtonpost',\n",
    "         'newyorktimes', 'npr', 'reuters',\n",
    "         'newyorkpost', 'foxnews', 'nationalreview',\n",
    "         'breitbart']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cdd8249-695e-44cb-a316-4aa6135045c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating our publication test dataframe\n",
    "bert_pub_train = pd.concat([pd.read_csv(f'./data/{i}.csv').sample(3000) for i in names])\n",
    "bert_pub_train = bert_pub_train[['publication', 'content']]\n",
    "bert_pub_train['publication'] = [i.replace(' ', '').lower() for i in bert_pub_train['publication']]\n",
    "\n",
    "# removing cnn from our articles\n",
    "bert_pub_train['content'] = [i.replace('(' + 'CNN' + ')', '') for i in bert_pub_train['content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bea8a014-6688-4225-8846-b7529f11ed70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# creating our publication test dataframe\n",
    "bert_pub_test = pd.concat([pd.read_csv(f'./data/{i}.csv').sample(200) for i in names])\n",
    "bert_pub_test = bert_pub_test[['publication', 'content']]\n",
    "bert_pub_test['publication'] = [i.replace(' ', '').lower() for i in bert_pub_test['publication']]\n",
    "\n",
    "# removing cnn from our articles\n",
    "bert_pub_test['content'] = [i.replace('(' + 'CNN' + ')', '') for i in bert_pub_test['content']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30da7b81-2de5-41df-baa4-63f50520ca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a smaller dataframe for parameter testing\n",
    "bert_pub_train_test = pd.concat([pd.read_csv(f'./data/{i}.csv').sample(500) for i in names])\n",
    "bert_pub_train_test = bert_pub_train_test[['publication', 'content']]\n",
    "bert_pub_train_test['publication'] = [i.replace(' ', '').lower() for i in bert_pub_train_test['publication']]\n",
    "\n",
    "# removing cnn from our articles\n",
    "bert_pub_train_test['content'] = [i.replace('(' + 'CNN' + ')', '') for i in bert_pub_train_test['content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dc859c59-13d2-4ddb-9bbe-d4ad2e584ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating our publication test dataframe, didn't sample as we need to filter for political articles\n",
    "bert_bias_test = pd.concat([pd.read_csv(f'./data/{i}.csv') for i in names])\n",
    "bert_bias_test['publication'] = [i.replace(' ', '').lower() for i in bert_bias_test['publication']]\n",
    "bert_bias_test = bert_bias_test[['publication', 'content']]\n",
    "\n",
    "#removing cnn from out articles\n",
    "bert_bias_test['content'] = [i.replace('(' + 'CNN' + ')', '') for i in bert_bias_test['content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c0c26cac-9019-4d84-9a87-64ce2d1f4f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating list of political words to filter poltical topics\n",
    "poli_names = ['Trump', 'Hillary', 'Clinton', 'Obama', 'Biden', 'Sanders', 'GOP', 'Republicans', 'Democrats', 'Liberals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46cfb5ce-83eb-4e18-be36-680abb3de2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering for political articles\n",
    "bert_bias_filtered = bert_bias_test[bert_bias_test['content'].str.contains('Trump|Hillary|Clinton|Obama|Biden|Sanders|GOP|Republicans|Democrats|Liberals')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "47755cec-1a93-4c28-bf65-3306af6a84a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating our final bias_test df\n",
    "bert_bias_test = bert_bias_filtered.copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93f8718d-d483-4aee-90dd-40134a6fd679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# turning our publications into numbers\n",
    "bert_pub_train['publication'].replace({'buzzfeednews' : 0, 'cnn' : 1, 'vox' : 2,\n",
    "                                 'guardian' : 3, 'atlantic' : 4, 'washingtonpost' : 5, \n",
    "                                 'newyorktimes' : 6, 'npr' : 7, 'reuters' : 8, \n",
    "                                 'newyorkpost' : 9, 'foxnews' :10, 'nationalreview' : 11, \n",
    "                                 'breitbart' : 12}, inplace= True)\n",
    "\n",
    "\n",
    "# bert_pub_test['publication'].replace({'buzzfeednews' : 0, 'cnn' : 1, 'vox' : 2,\n",
    "#                                  'guardian' : 3, 'atlantic' : 4, 'washingtonpost' : 5, \n",
    "#                                  'newyorktimes' : 6, 'npr' : 7, 'reuters' : 8, \n",
    "#                                  'newyorkpost' : 9, 'foxnews' :10, 'nationalreview' : 11, \n",
    "#                                  'breitbart' : 12}, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e92a09a2-263c-42aa-b5b0-e5d1b3613fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_pub_train_test['publication'].replace({'buzzfeednews' : 0, 'cnn' : 1, 'vox' : 2,\n",
    "                                 'guardian' : 3, 'atlantic' : 4, 'washingtonpost' : 5, \n",
    "                                 'newyorktimes' : 6, 'npr' : 7, 'reuters' : 8, \n",
    "                                 'newyorkpost' : 9, 'foxnews' :10, 'nationalreview' : 11, \n",
    "                                 'breitbart' : 12}, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c5484d8e-1bd5-4eaa-91df-fc2302309e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning our publications into bias numbers\n",
    "bert_bias_test['publication'].replace({'buzzfeednews' : 0, 'cnn' : 0, 'vox' : 0,\n",
    "                                 'guardian' : 1, 'atlantic' : 1, 'washingtonpost' : 1, \n",
    "                                 'newyorktimes' : 1, 'npr' : 2, 'reuters' : 2, \n",
    "                                 'newyorkpost' : 3, 'foxnews' :4, 'nationalreview' : 4, \n",
    "                                 'breitbart' : 4}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d6adce75-beb4-4973-a86e-eb27fe80ca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling from our df to create testing and training data\n",
    "bert_bias_train = pd.concat([bert_bias_test.loc[bert_bias_test['publication'] == i].sample(2500) for i in bert_bias_test['publication'].value_counts().index])\n",
    "bert_bias_test = pd.concat([bert_bias_test.loc[bert_bias_test['publication'] == i].sample(200) for i in bert_bias_test['publication'].value_counts().index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "572abb12-af73-4dd4-bfd5-7edde2438975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaimng our column \n",
    "bert_bias_train.rename(columns = {'publication': 'bias'}, inplace=True)\n",
    "bert_bias_test.rename(columns = {'publication': 'bias'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6173d4c-f79e-43b4-b419-c08a544f3e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving our pub test/train\n",
    "bert_pub_train.to_csv('./data/bert_pub_train.csv')\n",
    "#bert_pub_test.to_csv('./data/bert_pub_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "92461de1-c3d3-4e89-918e-fd60b4ae011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving our bias test/train\n",
    "bert_bias_train.to_csv('./data/bert_bias_train.csv')\n",
    "bert_bias_test.to_csv('./data/bert_bias_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d77dc7c4-517e-46c7-94c9-5a0255ac6c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving parameter testing dataframe\n",
    "bert_pub_train_test.to_csv('./data/bert_pub_train_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10267cb7-4bc9-46ab-80f0-d67fdb189358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
